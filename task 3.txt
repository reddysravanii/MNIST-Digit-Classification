1.KNN ALGORITHM :



 %pip install seaborn

import numpy as np

from sklearn.model_selection import train_test_split

from sklearn.neighbors import KNeighborsClassifier

from sklearn.datasets import load_digits

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

import matplotlib.pyplot as plt

import seaborn as sns

# 1. Load the Dataset

digits = load_digits()

X = digits.data

y = digits.target



# 2. Visualize a Few Samples (Optional)

fig, axes = plt.subplots(2, 5, figsize=(10, 5))

for i, ax in enumerate(axes.ravel()):

    ax.imshow(digits.images[i], cmap=plt.cm.gray_r)    

ax.set_title(f"Label: {digits.target[i]}")

    ax.axis('off')

plt.tight_layout()

plt.show()



# 3. Split the Data into Training and Testing Sets

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)



# 4. Initialize the KNN Classifier

# You can experiment with different values of 'n_neighbors' (k)

knn = KNeighborsClassifier(n_neighbors=5)



# 5. Train the KNN Classifier

knn.fit(X_train, y_train)



# 6. Make Predictions on the Test Set

y_pred = knn.predict(X_test)



# 7. Evaluate the Model

accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy of KNN: {accuracy:.4f}")



print("\nClassification Report:")

print(classification_report(y_test, y_pred))



# 8. Confusion Matrix (Optional but Recommended for Detailed Evaluation)

cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8, 6))

sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',

            xticklabels=digits.target_names, yticklabels=digits.target_names)

plt.xlabel('Predicted Label')

plt.ylabel('True Label')

plt.title('Confusion Matrix')

plt.show()



2.SVM ALGORITHM:



import numpy as np

from sklearn.model_selection import train_test_split

from sklearn import svm

from sklearn.metrics import accuracy_score

from sklearn.datasets import load_digits  



# 1. Load the Dataset

digits = load_digits() 

X = digits.data

y = digits.target



# Scale the data (pixel values are between 0 and 255)

X = X / 255.0



# Split the data into training and testing sets

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)



# Initialize the SVM classifier

clf = svm.SVC(kernel='rbf', gamma='scale', C=1.0)



# Train the SVM classifier on the training data

print("Training the SVM model...")

clf.fit(X_train, y_train)

print("Training complete.")



# Make predictions on the test data

print("Making predictions on the test set...")

y_pred = clf.predict(X_test)

print("Predictions complete.")

# Evaluate the accuracy of the model

accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy of the SVM classifier: {accuracy:.4f}")



3.RANDOM FOREST CLASSIFIER:



import numpy as np

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

import seaborn as sns

from sklearn.datasets import load_digits



# 1. Load the Dataset

digits = load_digits() 

X = digits.data

y = digits.target





# 2. Visualize a few samples

n_samples_to_show = 5

plt.figure(figsize=(10, 2))

for i in range(n_samples_to_show):

    plt.subplot(1, n_samples_to_show, i + 1)

   # plt.imshow(X[i].reshape(28, 28), cmap='gray')

    plt.title(f"Digit: {y[i]}")

    plt.axis('off')

plt.show()  # Added plt.show()



# 3. Split the data into training and testing sets

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)



print(f"Training data shape: {X_train.shape}")

print(f"Testing data shape: {X_test.shape}")

# 4. Initialize and train the Random Forest Classifier

# You can adjust hyperparameters like n_estimators (number of trees),

# max_depth, min_samples_split, etc.

random_forest = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)

print("Training the Random Forest Classifier...", flush=True)  # Added flush=True

random_forest.fit(X_train, y_train)

print("Training complete.", flush=True)  # Added flush=True



# 5. Make predictions on the test set

y_pred = random_forest.predict(X_test)



# 6. Evaluate the model

accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy on the test set: {accuracy:.4f}")



print("\nClassification Report:")

print(classification_report(y_test, y_pred))



# 7. Display the confusion matrix

cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(10, 8))

sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',

            xticklabels=np.unique(y), yticklabels=np.unique(y))

plt.xlabel('Predicted Label')

plt.ylabel('True Label')

plt.title('Confusion Matrix')

plt.show()  # Added plt.show()



# 8. (Optional) Visualize some misclassified samples

misclassified_indices = np.where(y_pred != y_test)[0]

n_misclassified_to_show = 5

if len(misclassified_indices) > 0:

    print("\nSome Misclassified Samples:")

    plt.figure(figsize=(10, 2))

    for i, index in enumerate(misclassified_indices[:n_misclassified_to_show]):

        plt.subplot(1, n_misclassified_to_show, i + 1)

      #  plt.imshow(X_test[index].reshape(28, 28), cmap='gray')

        plt.title(f"True: {y_test[index]}, Predicted: {y_pred[index]}")

        plt.axis('off')

    plt.show()  # Added plt.show()

else:

    print("\nNo misclassified samples found (within the displayed limit).")
